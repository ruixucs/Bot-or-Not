{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Bot-or-Not Challenge - Bot Detection System (Enhanced v2)\n",
        "\n",
        "Multi-layer bot detection pipeline:\n",
        "1. **Basic Features**: User profile + posting behavior + content statistics\n",
        "2. **Advanced Temporal Features**: Time DNA sequences + inter-arrival distribution fitting + session analysis\n",
        "3. **Text Stylometry Features**: N-gram repetition + Jaccard similarity + Zipf deviation + compression ratio\n",
        "4. **Deep NLP Features**: Sentence embeddings + LLM perplexity\n",
        "5. **Cross-user Features**: Inter-user similarity + HDBSCAN clustering\n",
        "6. **Threshold Optimization**: Custom scoring (+4 TP, -1 FN, -2 FP) with fine-grained search\n",
        "7. **Stacking Ensemble**: XGBoost + LightGBM + CatBoost (Layer 1) -> LogisticRegression (Layer 2)"
      ],
      "id": "e22b4a9a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "!pip install xgboost lightgbm catboost sentence-transformers transformers torch scikit-learn pandas numpy scipy hdbscan optuna -q"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "6e0b5ef7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import json\n",
        "import re\n",
        "import os\n",
        "import zlib\n",
        "import warnings\n",
        "from datetime import datetime\n",
        "from collections import Counter\n",
        "from functools import partial\n",
        "from itertools import combinations\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "from scipy.spatial.distance import cosine\n",
        "from scipy.optimize import curve_fit\n",
        "\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
        "from sklearn.metrics import (accuracy_score, precision_recall_fscore_support,\n",
        "                             classification_report, confusion_matrix)\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "import hdbscan\n",
        "\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "from catboost import CatBoostClassifier\n",
        "\n",
        "import optuna\n",
        "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "print(f\"GPU available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "2d6b6403"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 1: Data Loading\n",
        "\n",
        "Upload the dataset files to Colab, then load them."
      ],
      "id": "b83f5b25"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Mount Google Drive and set DATA_DIR\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "DATA_DIR = '/content/drive/MyDrive/bot or not'"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "9f2b5e0f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def load_dataset(json_path, bots_path=None):\n",
        "    \"\"\"Load a dataset from JSON and optionally load bot labels.\"\"\"\n",
        "    with open(json_path, 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    posts_df = pd.DataFrame(data['posts'])\n",
        "    users_df = pd.DataFrame(data['users'])\n",
        "    metadata = {\n",
        "        'id': data['id'],\n",
        "        'lang': data['lang'],\n",
        "        'metadata': data['metadata']\n",
        "    }\n",
        "\n",
        "    # Load bot labels if available (practice datasets)\n",
        "    bot_ids = set()\n",
        "    if bots_path and os.path.exists(bots_path):\n",
        "        with open(bots_path, 'r') as f:\n",
        "            bot_ids = set(line.strip() for line in f if line.strip())\n",
        "        users_df['is_bot'] = users_df['id'].isin(bot_ids).astype(int)\n",
        "    else:\n",
        "        users_df['is_bot'] = -1  # Unknown\n",
        "\n",
        "    print(f\"Dataset {metadata['id']} ({metadata['lang']}): \"\n",
        "          f\"{len(users_df)} users, {len(posts_df)} posts, {len(bot_ids)} known bots\")\n",
        "    return posts_df, users_df, bot_ids, metadata\n",
        "\n",
        "\n",
        "# Load all practice datasets\n",
        "datasets = {}\n",
        "for ds_id in [30, 31, 32, 33]:\n",
        "    json_path = os.path.join(DATA_DIR, f'dataset.posts&users.{ds_id}.json')\n",
        "    bots_path = os.path.join(DATA_DIR, f'dataset.bots.{ds_id}.txt')\n",
        "    if os.path.exists(json_path):\n",
        "        posts_df, users_df, bot_ids, meta = load_dataset(json_path, bots_path)\n",
        "        datasets[ds_id] = {\n",
        "            'posts': posts_df, 'users': users_df,\n",
        "            'bot_ids': bot_ids, 'meta': meta\n",
        "        }\n",
        "\n",
        "print(f\"\\nLoaded {len(datasets)} datasets: {list(datasets.keys())}\")\n",
        "print(f\"English datasets: {[k for k,v in datasets.items() if v['meta']['lang']=='en']}\")\n",
        "print(f\"French datasets: {[k for k,v in datasets.items() if v['meta']['lang']=='fr']}\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "78f64116"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 2: Feature Engineering (Layer 1 - Basic Features)\n",
        "\n",
        "Extract 35+ features from user profiles, posting behavior, and content statistics."
      ],
      "id": "2e898dfc"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "EMOJI_PATTERN = re.compile(r'[\\U00010000-\\U0010ffff]', flags=re.UNICODE)\n",
        "\n",
        "# ============================================================\n",
        "# 2a. User Profile Features\n",
        "# ============================================================\n",
        "def extract_user_profile_features(user):\n",
        "    feats = {}\n",
        "\n",
        "    # Basic stats (from dataset)\n",
        "    feats['tweet_count'] = user['tweet_count']\n",
        "    feats['z_score'] = user['z_score']\n",
        "\n",
        "    # Username features\n",
        "    uname = str(user.get('username', ''))\n",
        "    feats['username_length'] = len(uname)\n",
        "    feats['username_digit_ratio'] = sum(c.isdigit() for c in uname) / max(len(uname), 1)\n",
        "    feats['username_underscore_count'] = uname.count('_')\n",
        "    feats['username_upper_ratio'] = sum(c.isupper() for c in uname) / max(len(uname), 1)\n",
        "    feats['username_has_numbers'] = int(any(c.isdigit() for c in uname))\n",
        "\n",
        "    # Name features\n",
        "    name = str(user.get('name', '') or '')\n",
        "    feats['name_length'] = len(name)\n",
        "    feats['name_emoji_count'] = len(EMOJI_PATTERN.findall(name))\n",
        "    feats['name_word_count'] = len(name.split())\n",
        "\n",
        "    # Description features\n",
        "    desc = str(user.get('description', '') or '')\n",
        "    feats['has_description'] = int(bool(desc.strip()))\n",
        "    feats['description_length'] = len(desc)\n",
        "    feats['description_emoji_count'] = len(EMOJI_PATTERN.findall(desc))\n",
        "    feats['description_word_count'] = len(desc.split()) if desc.strip() else 0\n",
        "    feats['description_hashtag_count'] = desc.count('#')\n",
        "    feats['description_url_count'] = len(re.findall(r'https?://\\S+', desc))\n",
        "    feats['description_pipe_count'] = desc.count('|')  # Bio separators like \"Gamer | Streamer\"\n",
        "\n",
        "    # Location features\n",
        "    loc = str(user.get('location', '') or '')\n",
        "    feats['has_location'] = int(bool(loc.strip()))\n",
        "    feats['location_length'] = len(loc)\n",
        "\n",
        "    return feats\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 2b. Posting Behavior Features\n",
        "# ============================================================\n",
        "def extract_behavioral_features(user_posts):\n",
        "    feats = {}\n",
        "    n = len(user_posts)\n",
        "\n",
        "    if n < 2:\n",
        "        return {k: 0 for k in [\n",
        "            'posting_frequency', 'time_span_hours',\n",
        "            'avg_interval', 'std_interval', 'min_interval', 'max_interval',\n",
        "            'cv_interval', 'median_interval',\n",
        "            'interval_skewness', 'interval_kurtosis',\n",
        "            'hour_entropy', 'night_post_ratio', 'morning_post_ratio',\n",
        "            'evening_post_ratio',\n",
        "            'burst_count_60s', 'burst_count_300s',\n",
        "            'unique_hours', 'unique_days',\n",
        "            'weekend_ratio', 'posts_per_day_std',\n",
        "            'max_posts_in_hour', 'regularity_score'\n",
        "        ]}\n",
        "\n",
        "    timestamps = pd.to_datetime(user_posts['created_at']).sort_values().reset_index(drop=True)\n",
        "\n",
        "    # Time span\n",
        "    time_span = (timestamps.max() - timestamps.min()).total_seconds() / 3600\n",
        "    feats['time_span_hours'] = time_span\n",
        "    feats['posting_frequency'] = n / max(time_span, 0.01)\n",
        "\n",
        "    # Time intervals (in seconds)\n",
        "    intervals = timestamps.diff().dropna().dt.total_seconds().values\n",
        "    feats['avg_interval'] = np.mean(intervals)\n",
        "    feats['std_interval'] = np.std(intervals)\n",
        "    feats['min_interval'] = np.min(intervals)\n",
        "    feats['max_interval'] = np.max(intervals)\n",
        "    feats['median_interval'] = np.median(intervals)\n",
        "    feats['cv_interval'] = np.std(intervals) / max(np.mean(intervals), 0.01)\n",
        "\n",
        "    # Distribution shape\n",
        "    if len(intervals) >= 4:\n",
        "        feats['interval_skewness'] = float(stats.skew(intervals))\n",
        "        feats['interval_kurtosis'] = float(stats.kurtosis(intervals))\n",
        "    else:\n",
        "        feats['interval_skewness'] = 0\n",
        "        feats['interval_kurtosis'] = 0\n",
        "\n",
        "    # Hour distribution entropy\n",
        "    hours = timestamps.dt.hour\n",
        "    hour_counts = hours.value_counts(normalize=True)\n",
        "    feats['hour_entropy'] = float(stats.entropy(hour_counts))\n",
        "\n",
        "    # Time-of-day ratios\n",
        "    feats['night_post_ratio'] = hours.between(0, 5).mean()\n",
        "    feats['morning_post_ratio'] = hours.between(6, 11).mean()\n",
        "    feats['evening_post_ratio'] = hours.between(18, 23).mean()\n",
        "\n",
        "    # Burst detection\n",
        "    feats['burst_count_60s'] = int((intervals < 60).sum())\n",
        "    feats['burst_count_300s'] = int((intervals < 300).sum())\n",
        "\n",
        "    # Activity spread\n",
        "    feats['unique_hours'] = hours.nunique()\n",
        "    feats['unique_days'] = timestamps.dt.date.nunique()\n",
        "    feats['weekend_ratio'] = timestamps.dt.dayofweek.isin([5, 6]).mean()\n",
        "\n",
        "    # Posts per day variability\n",
        "    posts_per_day = timestamps.dt.date.value_counts()\n",
        "    feats['posts_per_day_std'] = posts_per_day.std() if len(posts_per_day) > 1 else 0\n",
        "\n",
        "    # Max posts in any single hour\n",
        "    hour_day = timestamps.dt.floor('h')\n",
        "    feats['max_posts_in_hour'] = hour_day.value_counts().max()\n",
        "\n",
        "    # Regularity score: how \"clock-like\" the posting is\n",
        "    # Low std of intervals relative to mean = very regular\n",
        "    feats['regularity_score'] = 1.0 / (1.0 + feats['cv_interval'])\n",
        "\n",
        "    return feats\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 2c. Content Features\n",
        "# ============================================================\n",
        "def extract_content_features(user_posts):\n",
        "    feats = {}\n",
        "    texts = user_posts['text'].dropna().tolist()\n",
        "\n",
        "    if not texts:\n",
        "        return {k: 0 for k in [\n",
        "            'avg_length_chars', 'avg_length_words', 'std_length_chars',\n",
        "            'vocabulary_richness', 'hapax_ratio',\n",
        "            'avg_hashtags', 'avg_urls', 'avg_mentions',\n",
        "            'emoji_rate', 'avg_exclamation', 'avg_question',\n",
        "            'avg_uppercase_ratio', 'avg_punctuation_ratio',\n",
        "            'duplicate_ratio', 'near_duplicate_ratio',\n",
        "            'avg_sentence_count', 'avg_word_length',\n",
        "            'link_tweet_ratio', 'retweet_ratio'\n",
        "        ]}\n",
        "\n",
        "    # Length features\n",
        "    char_lens = [len(t) for t in texts]\n",
        "    word_lens = [len(t.split()) for t in texts]\n",
        "    feats['avg_length_chars'] = np.mean(char_lens)\n",
        "    feats['avg_length_words'] = np.mean(word_lens)\n",
        "    feats['std_length_chars'] = np.std(char_lens)\n",
        "\n",
        "    # Vocabulary richness\n",
        "    all_words = ' '.join(texts).lower().split()\n",
        "    unique_words = set(all_words)\n",
        "    feats['vocabulary_richness'] = len(unique_words) / max(len(all_words), 1)\n",
        "    # Hapax legomena ratio (words appearing only once)\n",
        "    word_counts = Counter(all_words)\n",
        "    feats['hapax_ratio'] = sum(1 for c in word_counts.values() if c == 1) / max(len(unique_words), 1)\n",
        "\n",
        "    # Entity counts\n",
        "    feats['avg_hashtags'] = np.mean([t.count('#') for t in texts])\n",
        "    feats['avg_urls'] = np.mean([len(re.findall(r'https?://\\S+|t\\.co/\\S+', t)) for t in texts])\n",
        "    feats['avg_mentions'] = np.mean([t.count('@') for t in texts])\n",
        "    feats['emoji_rate'] = np.mean([len(EMOJI_PATTERN.findall(t)) for t in texts])\n",
        "\n",
        "    # Punctuation\n",
        "    feats['avg_exclamation'] = np.mean([t.count('!') for t in texts])\n",
        "    feats['avg_question'] = np.mean([t.count('?') for t in texts])\n",
        "    feats['avg_uppercase_ratio'] = np.mean([\n",
        "        sum(c.isupper() for c in t) / max(len(t), 1) for t in texts\n",
        "    ])\n",
        "    feats['avg_punctuation_ratio'] = np.mean([\n",
        "        sum(not c.isalnum() and not c.isspace() for c in t) / max(len(t), 1)\n",
        "        for t in texts\n",
        "    ])\n",
        "\n",
        "    # Duplicate / near-duplicate analysis\n",
        "    unique_texts = set(texts)\n",
        "    feats['duplicate_ratio'] = 1 - len(unique_texts) / max(len(texts), 1)\n",
        "\n",
        "    near_dup = 0\n",
        "    total_pairs = 0\n",
        "    sample = texts[:50]\n",
        "    for i in range(len(sample)):\n",
        "        wi = set(sample[i].lower().split())\n",
        "        for j in range(i + 1, len(sample)):\n",
        "            wj = set(sample[j].lower().split())\n",
        "            if wi and wj:\n",
        "                jaccard = len(wi & wj) / len(wi | wj)\n",
        "                if jaccard > 0.8:\n",
        "                    near_dup += 1\n",
        "            total_pairs += 1\n",
        "    feats['near_duplicate_ratio'] = near_dup / max(total_pairs, 1)\n",
        "\n",
        "    # Sentence & word complexity\n",
        "    feats['avg_sentence_count'] = np.mean([\n",
        "        len(re.split(r'[.!?]+', t)) for t in texts\n",
        "    ])\n",
        "    feats['avg_word_length'] = np.mean([len(w) for w in all_words]) if all_words else 0\n",
        "\n",
        "    # Link tweets ratio\n",
        "    feats['link_tweet_ratio'] = np.mean([\n",
        "        1 if re.search(r'https?://|t\\.co/', t) else 0 for t in texts\n",
        "    ])\n",
        "\n",
        "    # Retweet-like pattern ratio\n",
        "    feats['retweet_ratio'] = np.mean([\n",
        "        1 if t.strip().startswith('RT ') or t.strip().startswith('rt ') else 0\n",
        "        for t in texts\n",
        "    ])\n",
        "\n",
        "    return feats\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Combine all basic features for one dataset\n",
        "# ============================================================\n",
        "def extract_all_basic_features(posts_df, users_df):\n",
        "    \"\"\"Extract all Layer-1 features for every user in a dataset.\"\"\"\n",
        "    all_feats = []\n",
        "\n",
        "    for _, user in users_df.iterrows():\n",
        "        uid = user['id']\n",
        "        user_posts = posts_df[posts_df['author_id'] == uid]\n",
        "\n",
        "        f = {'user_id': uid}\n",
        "        f.update(extract_user_profile_features(user))\n",
        "        f.update(extract_behavioral_features(user_posts))\n",
        "        f.update(extract_content_features(user_posts))\n",
        "        all_feats.append(f)\n",
        "\n",
        "    df = pd.DataFrame(all_feats)\n",
        "    return df\n",
        "\n",
        "print(\"Feature extraction functions defined.\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "44779854"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Extract basic features for all datasets\n",
        "basic_features = {}\n",
        "for ds_id, ds in datasets.items():\n",
        "    print(f\"\\nExtracting basic features for dataset {ds_id}...\")\n",
        "    feats = extract_all_basic_features(ds['posts'], ds['users'])\n",
        "    feats = feats.merge(ds['users'][['id', 'is_bot']], left_on='user_id', right_on='id', how='left')\n",
        "    feats.drop(columns=['id'], inplace=True)\n",
        "    basic_features[ds_id] = feats\n",
        "    print(f\"  Shape: {feats.shape}, Bots: {(feats['is_bot']==1).sum()}, Humans: {(feats['is_bot']==0).sum()}\")\n",
        "\n",
        "print(\"\\nBasic feature extraction complete!\")\n",
        "basic_features[30].head()"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "3c31e971"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 2b: Advanced Temporal Features (NEW)\n",
        "\n",
        "Inspired by DARPA Bot Challenge winners and MulBot paper:\n",
        "- **Time DNA Sequences**: Encode posting times as character sequences, compute self-similarity\n",
        "- **Inter-arrival Distribution Fitting**: Fit exponential distribution, measure goodness-of-fit (bots are more regular)\n",
        "- **Session Analysis**: Define activity sessions, compute session-level statistics"
      ],
      "id": "9781fdf9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# 2d. Advanced Temporal Features\n",
        "# ============================================================\n",
        "\n",
        "def encode_time_dna(timestamps, resolution='hour'):\n",
        "    \"\"\"\n",
        "    Encode posting timestamps as a 'DNA' string sequence (Cresci et al.).\n",
        "    Each character represents a time bucket. We then measure self-similarity.\n",
        "    \"\"\"\n",
        "    if resolution == 'hour':\n",
        "        return ''.join([chr(ord('A') + t.hour) for t in timestamps])\n",
        "    elif resolution == 'minute_bucket':\n",
        "        # 10-minute buckets (0-5 -> A-F per hour)\n",
        "        return ''.join([chr(ord('A') + (t.hour * 6 + t.minute // 10)) for t in timestamps])\n",
        "    return ''\n",
        "\n",
        "\n",
        "def dna_self_similarity(dna_seq, k=3):\n",
        "    \"\"\"Compute self-similarity of a DNA sequence using k-mer overlap.\"\"\"\n",
        "    if len(dna_seq) < k + 1:\n",
        "        return 0.0\n",
        "    kmers = [dna_seq[i:i+k] for i in range(len(dna_seq) - k + 1)]\n",
        "    unique_kmers = set(kmers)\n",
        "    return 1.0 - (len(unique_kmers) / max(len(kmers), 1))\n",
        "\n",
        "\n",
        "def extract_advanced_temporal_features(user_posts):\n",
        "    \"\"\"Extract advanced temporal features per user.\"\"\"\n",
        "    feats = {}\n",
        "    n = len(user_posts)\n",
        "\n",
        "    default_feats = {\n",
        "        'time_dna_self_sim_hour': 0, 'time_dna_self_sim_minute': 0,\n",
        "        'time_dna_unique_3gram_ratio': 0,\n",
        "        'iat_exponential_ks_stat': 0, 'iat_exponential_ks_pvalue': 1.0,\n",
        "        'iat_gini_coefficient': 0,\n",
        "        'iat_benford_deviation': 0,\n",
        "        'session_count': 0, 'avg_session_length': 0, 'max_session_length': 0,\n",
        "        'avg_inter_session_gap': 0, 'session_regularity': 0,\n",
        "        'posting_acceleration': 0,\n",
        "        'longest_active_streak_hours': 0,\n",
        "        'minute_entropy': 0,\n",
        "        'day_of_week_entropy': 0,\n",
        "    }\n",
        "\n",
        "    if n < 3:\n",
        "        return default_feats\n",
        "\n",
        "    try:\n",
        "        timestamps = pd.to_datetime(user_posts['created_at']).sort_values().reset_index(drop=True)\n",
        "    except:\n",
        "        return default_feats\n",
        "\n",
        "    # --- Time DNA Sequences ---\n",
        "    dna_hour = encode_time_dna(timestamps, 'hour')\n",
        "    dna_minute = encode_time_dna(timestamps, 'minute_bucket')\n",
        "    feats['time_dna_self_sim_hour'] = dna_self_similarity(dna_hour, k=3)\n",
        "    feats['time_dna_self_sim_minute'] = dna_self_similarity(dna_minute, k=3)\n",
        "\n",
        "    # 3-gram uniqueness ratio for time DNA\n",
        "    if len(dna_hour) >= 4:\n",
        "        kmers = [dna_hour[i:i+3] for i in range(len(dna_hour) - 2)]\n",
        "        feats['time_dna_unique_3gram_ratio'] = len(set(kmers)) / max(len(kmers), 1)\n",
        "    else:\n",
        "        feats['time_dna_unique_3gram_ratio'] = 0\n",
        "\n",
        "    # --- Inter-arrival Time Distribution Fitting ---\n",
        "    intervals = timestamps.diff().dropna().dt.total_seconds().values\n",
        "    intervals = intervals[intervals > 0]  # Remove zero intervals\n",
        "\n",
        "    if len(intervals) >= 5:\n",
        "        # KS test against exponential distribution (human posting tends to be heavy-tailed)\n",
        "        try:\n",
        "            loc, scale = stats.expon.fit(intervals, floc=0)\n",
        "            ks_stat, ks_pvalue = stats.kstest(intervals, 'expon', args=(0, scale))\n",
        "            feats['iat_exponential_ks_stat'] = float(ks_stat)\n",
        "            feats['iat_exponential_ks_pvalue'] = float(ks_pvalue)\n",
        "        except:\n",
        "            feats['iat_exponential_ks_stat'] = 0\n",
        "            feats['iat_exponential_ks_pvalue'] = 1.0\n",
        "\n",
        "        # Gini coefficient of intervals (measures inequality; bots tend to be more equal = lower Gini)\n",
        "        sorted_intervals = np.sort(intervals)\n",
        "        n_int = len(sorted_intervals)\n",
        "        index = np.arange(1, n_int + 1)\n",
        "        feats['iat_gini_coefficient'] = float(\n",
        "            (2 * np.sum(index * sorted_intervals) / (n_int * np.sum(sorted_intervals))) - (n_int + 1) / n_int\n",
        "        ) if np.sum(sorted_intervals) > 0 else 0\n",
        "\n",
        "        # Benford's law deviation on leading digit of intervals\n",
        "        leading_digits = [int(str(abs(int(x)))[0]) for x in intervals if x >= 1]\n",
        "        if leading_digits:\n",
        "            digit_counts = Counter(leading_digits)\n",
        "            total = sum(digit_counts.values())\n",
        "            benford_expected = {d: np.log10(1 + 1/d) for d in range(1, 10)}\n",
        "            deviation = sum(\n",
        "                abs(digit_counts.get(d, 0) / total - benford_expected[d])\n",
        "                for d in range(1, 10)\n",
        "            )\n",
        "            feats['iat_benford_deviation'] = float(deviation)\n",
        "        else:\n",
        "            feats['iat_benford_deviation'] = 0\n",
        "    else:\n",
        "        feats['iat_exponential_ks_stat'] = 0\n",
        "        feats['iat_exponential_ks_pvalue'] = 1.0\n",
        "        feats['iat_gini_coefficient'] = 0\n",
        "        feats['iat_benford_deviation'] = 0\n",
        "\n",
        "    # --- Session Analysis ---\n",
        "    SESSION_GAP_SECONDS = 1800  # 30-minute gap defines a new session\n",
        "    if len(intervals) >= 1:\n",
        "        session_breaks = np.where(intervals > SESSION_GAP_SECONDS)[0]\n",
        "        session_count = len(session_breaks) + 1\n",
        "        feats['session_count'] = session_count\n",
        "\n",
        "        # Compute session lengths (number of posts per session)\n",
        "        session_starts = np.concatenate([[0], session_breaks + 1])\n",
        "        session_ends = np.concatenate([session_breaks + 1, [len(timestamps)]])\n",
        "        session_lengths = session_ends - session_starts\n",
        "\n",
        "        feats['avg_session_length'] = float(np.mean(session_lengths))\n",
        "        feats['max_session_length'] = float(np.max(session_lengths))\n",
        "\n",
        "        # Inter-session gaps\n",
        "        if session_count > 1:\n",
        "            inter_session_gaps = intervals[session_breaks]\n",
        "            feats['avg_inter_session_gap'] = float(np.mean(inter_session_gaps))\n",
        "            feats['session_regularity'] = float(\n",
        "                np.std(inter_session_gaps) / max(np.mean(inter_session_gaps), 0.01)\n",
        "            )\n",
        "        else:\n",
        "            feats['avg_inter_session_gap'] = 0\n",
        "            feats['session_regularity'] = 0\n",
        "    else:\n",
        "        feats['session_count'] = 1\n",
        "        feats['avg_session_length'] = n\n",
        "        feats['max_session_length'] = n\n",
        "        feats['avg_inter_session_gap'] = 0\n",
        "        feats['session_regularity'] = 0\n",
        "\n",
        "    # --- Posting Acceleration ---\n",
        "    # Compare posting frequency in first half vs second half of time span\n",
        "    mid_idx = len(timestamps) // 2\n",
        "    if mid_idx > 0 and mid_idx < len(timestamps) - 1:\n",
        "        first_half_span = (timestamps.iloc[mid_idx] - timestamps.iloc[0]).total_seconds()\n",
        "        second_half_span = (timestamps.iloc[-1] - timestamps.iloc[mid_idx]).total_seconds()\n",
        "        freq_first = mid_idx / max(first_half_span, 1)\n",
        "        freq_second = (len(timestamps) - mid_idx) / max(second_half_span, 1)\n",
        "        feats['posting_acceleration'] = freq_second - freq_first\n",
        "    else:\n",
        "        feats['posting_acceleration'] = 0\n",
        "\n",
        "    # --- Longest Active Streak ---\n",
        "    # How many hours was the user continuously active (at least 1 post per hour)\n",
        "    hour_bins = timestamps.dt.floor('h')\n",
        "    active_hours = sorted(hour_bins.unique())\n",
        "    if len(active_hours) > 1:\n",
        "        diffs = [(active_hours[i+1] - active_hours[i]).total_seconds() / 3600\n",
        "                 for i in range(len(active_hours) - 1)]\n",
        "        longest_streak = 1\n",
        "        current_streak = 1\n",
        "        for d in diffs:\n",
        "            if d <= 1.0:\n",
        "                current_streak += 1\n",
        "                longest_streak = max(longest_streak, current_streak)\n",
        "            else:\n",
        "                current_streak = 1\n",
        "        feats['longest_active_streak_hours'] = longest_streak\n",
        "    else:\n",
        "        feats['longest_active_streak_hours'] = 1\n",
        "\n",
        "    # --- Minute-level entropy ---\n",
        "    minutes = timestamps.dt.minute\n",
        "    minute_counts = minutes.value_counts(normalize=True)\n",
        "    feats['minute_entropy'] = float(stats.entropy(minute_counts))\n",
        "\n",
        "    # --- Day of week entropy ---\n",
        "    dow = timestamps.dt.dayofweek\n",
        "    dow_counts = dow.value_counts(normalize=True)\n",
        "    feats['day_of_week_entropy'] = float(stats.entropy(dow_counts))\n",
        "\n",
        "    return feats\n",
        "\n",
        "\n",
        "# Apply advanced temporal features to all datasets\n",
        "advanced_temporal_features = {}\n",
        "for ds_id, ds in datasets.items():\n",
        "    print(f\"\\nExtracting advanced temporal features for dataset {ds_id}...\")\n",
        "    results = []\n",
        "    for _, user in ds['users'].iterrows():\n",
        "        uid = user['id']\n",
        "        user_posts = ds['posts'][ds['posts']['author_id'] == uid]\n",
        "        f = {'user_id': uid}\n",
        "        f.update(extract_advanced_temporal_features(user_posts))\n",
        "        results.append(f)\n",
        "    advanced_temporal_features[ds_id] = pd.DataFrame(results)\n",
        "    print(f\"  Shape: {advanced_temporal_features[ds_id].shape}\")\n",
        "\n",
        "print(\"\\nAdvanced temporal feature extraction complete!\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "8b1124ef"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 2c: Text Stylometry Features (NEW)\n",
        "\n",
        "Inspired by TwiBot-22 baselines (Lee's compression ratio, Kantepe's text entropy):\n",
        "- **N-gram Repetition**: 2-gram and 3-gram repetition rates across tweets\n",
        "- **Pairwise Jaccard Similarity**: Average Jaccard between tweet pairs (bot tweets are template-like)\n",
        "- **Zipf's Law Deviation**: Human text follows Zipf's law; bots may deviate\n",
        "- **Compression Ratio**: How compressible the user's combined text is (bots = more compressible)\n",
        "- **Text Entropy**: Shannon entropy of character distribution\n",
        "- **Punctuation Pattern Features**: Bot punctuation usage is often more regular"
      ],
      "id": "0d347949"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# 2e. Text Stylometry Features\n",
        "# ============================================================\n",
        "\n",
        "def compute_ngram_repetition(texts, n=2):\n",
        "    \"\"\"Compute n-gram repetition rate across all tweets.\"\"\"\n",
        "    all_ngrams = []\n",
        "    for text in texts:\n",
        "        words = text.lower().split()\n",
        "        ngrams = [tuple(words[i:i+n]) for i in range(len(words) - n + 1)]\n",
        "        all_ngrams.extend(ngrams)\n",
        "    if not all_ngrams:\n",
        "        return 0.0\n",
        "    counts = Counter(all_ngrams)\n",
        "    repeated = sum(c for c in counts.values() if c > 1)\n",
        "    return repeated / max(len(all_ngrams), 1)\n",
        "\n",
        "\n",
        "def compute_zipf_deviation(texts):\n",
        "    \"\"\"\n",
        "    Measure deviation from Zipf's law.\n",
        "    Zipf's law: frequency of word rank r is proportional to 1/r.\n",
        "    Returns the mean squared error between observed and expected Zipf distribution.\n",
        "    \"\"\"\n",
        "    all_words = ' '.join(texts).lower().split()\n",
        "    if len(all_words) < 10:\n",
        "        return 0.0\n",
        "\n",
        "    word_counts = Counter(all_words)\n",
        "    frequencies = sorted(word_counts.values(), reverse=True)\n",
        "    ranks = np.arange(1, len(frequencies) + 1)\n",
        "    freq_arr = np.array(frequencies, dtype=float)\n",
        "    freq_arr /= freq_arr[0]  # Normalize to first rank\n",
        "\n",
        "    # Expected Zipf: f(r) = 1/r\n",
        "    expected = 1.0 / ranks\n",
        "    # Use only top 50 to avoid noise\n",
        "    limit = min(50, len(freq_arr))\n",
        "    mse = float(np.mean((freq_arr[:limit] - expected[:limit]) ** 2))\n",
        "    return mse\n",
        "\n",
        "\n",
        "def compute_text_entropy(text):\n",
        "    \"\"\"Compute Shannon entropy of character distribution.\"\"\"\n",
        "    if not text:\n",
        "        return 0.0\n",
        "    char_counts = Counter(text.lower())\n",
        "    total = sum(char_counts.values())\n",
        "    probs = [c / total for c in char_counts.values()]\n",
        "    return float(stats.entropy(probs))\n",
        "\n",
        "\n",
        "def compute_compression_ratio(texts):\n",
        "    \"\"\"\n",
        "    Compute compression ratio of combined text (Lee et al.).\n",
        "    More compressible = more repetitive = more bot-like.\n",
        "    \"\"\"\n",
        "    combined = ' '.join(texts).encode('utf-8')\n",
        "    if len(combined) < 10:\n",
        "        return 1.0\n",
        "    compressed = zlib.compress(combined)\n",
        "    return len(combined) / max(len(compressed), 1)\n",
        "\n",
        "\n",
        "def extract_stylometry_features(user_posts):\n",
        "    \"\"\"Extract text stylometry features per user.\"\"\"\n",
        "    feats = {}\n",
        "    texts = user_posts['text'].dropna().tolist()\n",
        "\n",
        "    default_feats = {\n",
        "        'ngram2_repetition': 0, 'ngram3_repetition': 0,\n",
        "        'pairwise_jaccard_mean': 0, 'pairwise_jaccard_std': 0,\n",
        "        'pairwise_jaccard_max': 0,\n",
        "        'zipf_deviation': 0,\n",
        "        'compression_ratio': 1.0,\n",
        "        'text_char_entropy': 0,\n",
        "        'punctuation_pattern_std': 0,\n",
        "        'sentence_length_cv': 0,\n",
        "        'avg_word_length_std': 0,\n",
        "        'unique_first_words_ratio': 0,\n",
        "        'url_pattern_regularity': 0,\n",
        "        'mention_diversity': 0,\n",
        "        'hashtag_diversity': 0,\n",
        "    }\n",
        "\n",
        "    if len(texts) < 3:\n",
        "        return default_feats\n",
        "\n",
        "    # --- N-gram Repetition ---\n",
        "    feats['ngram2_repetition'] = compute_ngram_repetition(texts, n=2)\n",
        "    feats['ngram3_repetition'] = compute_ngram_repetition(texts, n=3)\n",
        "\n",
        "    # --- Pairwise Jaccard Similarity ---\n",
        "    sample = texts[:50]\n",
        "    jaccard_scores = []\n",
        "    for i in range(len(sample)):\n",
        "        wi = set(sample[i].lower().split())\n",
        "        for j in range(i + 1, min(i + 10, len(sample))):  # Limit pairs for speed\n",
        "            wj = set(sample[j].lower().split())\n",
        "            if wi or wj:\n",
        "                jaccard_scores.append(len(wi & wj) / max(len(wi | wj), 1))\n",
        "    if jaccard_scores:\n",
        "        feats['pairwise_jaccard_mean'] = float(np.mean(jaccard_scores))\n",
        "        feats['pairwise_jaccard_std'] = float(np.std(jaccard_scores))\n",
        "        feats['pairwise_jaccard_max'] = float(np.max(jaccard_scores))\n",
        "    else:\n",
        "        feats['pairwise_jaccard_mean'] = 0\n",
        "        feats['pairwise_jaccard_std'] = 0\n",
        "        feats['pairwise_jaccard_max'] = 0\n",
        "\n",
        "    # --- Zipf's Law Deviation ---\n",
        "    feats['zipf_deviation'] = compute_zipf_deviation(texts)\n",
        "\n",
        "    # --- Compression Ratio ---\n",
        "    feats['compression_ratio'] = compute_compression_ratio(texts)\n",
        "\n",
        "    # --- Character Entropy ---\n",
        "    combined_text = ' '.join(texts)\n",
        "    feats['text_char_entropy'] = compute_text_entropy(combined_text)\n",
        "\n",
        "    # --- Punctuation Pattern Regularity ---\n",
        "    # Std of punctuation count per tweet (regular = bot-like)\n",
        "    punct_counts = [sum(1 for c in t if not c.isalnum() and not c.isspace()) for t in texts]\n",
        "    feats['punctuation_pattern_std'] = float(np.std(punct_counts)) if punct_counts else 0\n",
        "\n",
        "    # --- Sentence Length Coefficient of Variation ---\n",
        "    word_lengths = [len(t.split()) for t in texts]\n",
        "    mean_wl = np.mean(word_lengths)\n",
        "    feats['sentence_length_cv'] = float(np.std(word_lengths) / max(mean_wl, 0.01))\n",
        "\n",
        "    # --- Word Length Variability Across Tweets ---\n",
        "    avg_word_lens = [np.mean([len(w) for w in t.split()]) if t.split() else 0 for t in texts]\n",
        "    feats['avg_word_length_std'] = float(np.std(avg_word_lens))\n",
        "\n",
        "    # --- Unique First Words Ratio (template detection) ---\n",
        "    first_words = [t.split()[0].lower() if t.split() else '' for t in texts]\n",
        "    feats['unique_first_words_ratio'] = len(set(first_words)) / max(len(first_words), 1)\n",
        "\n",
        "    # --- URL/Mention/Hashtag Diversity ---\n",
        "    all_urls = [url for t in texts for url in re.findall(r'https?://\\S+|t\\.co/\\S+', t)]\n",
        "    feats['url_pattern_regularity'] = 1 - (len(set(all_urls)) / max(len(all_urls), 1)) if all_urls else 0\n",
        "\n",
        "    all_mentions = [m for t in texts for m in re.findall(r'@\\w+', t)]\n",
        "    feats['mention_diversity'] = len(set(all_mentions)) / max(len(all_mentions), 1) if all_mentions else 0\n",
        "\n",
        "    all_hashtags = [h for t in texts for h in re.findall(r'#\\w+', t)]\n",
        "    feats['hashtag_diversity'] = len(set(all_hashtags)) / max(len(all_hashtags), 1) if all_hashtags else 0\n",
        "\n",
        "    return feats\n",
        "\n",
        "\n",
        "# Apply stylometry features to all datasets\n",
        "stylometry_features = {}\n",
        "for ds_id, ds in datasets.items():\n",
        "    print(f\"\\nExtracting stylometry features for dataset {ds_id}...\")\n",
        "    results = []\n",
        "    for _, user in ds['users'].iterrows():\n",
        "        uid = user['id']\n",
        "        user_posts = ds['posts'][ds['posts']['author_id'] == uid]\n",
        "        f = {'user_id': uid}\n",
        "        f.update(extract_stylometry_features(user_posts))\n",
        "        results.append(f)\n",
        "    stylometry_features[ds_id] = pd.DataFrame(results)\n",
        "    print(f\"  Shape: {stylometry_features[ds_id].shape}\")\n",
        "\n",
        "print(\"\\nStylometry feature extraction complete!\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "29a0771b"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 3: Deep NLP Features (Layer 2)\n",
        "\n",
        "- **Sentence Transformer**: Compute per-user tweet embedding similarity (bot tweets may be too uniform or too random)\n",
        "- **GPT-2 Perplexity**: AI-generated text tends to have lower, more consistent perplexity"
      ],
      "id": "5edb2c6f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# 3a. Sentence Transformer Embedding Features\n",
        "# ============================================================\n",
        "# Using multilingual model so it works for both English and French\n",
        "EMBED_MODEL_NAME = 'paraphrase-multilingual-MiniLM-L12-v2'\n",
        "\n",
        "def extract_embedding_features(posts_df, users_df, model=None):\n",
        "    \"\"\"Compute per-user tweet embedding similarity features.\"\"\"\n",
        "    if model is None:\n",
        "        model = SentenceTransformer(EMBED_MODEL_NAME)\n",
        "\n",
        "    results = []\n",
        "    mean_embeddings = {}  # Store for cross-user analysis later\n",
        "\n",
        "    for _, user in users_df.iterrows():\n",
        "        uid = user['id']\n",
        "        user_posts = posts_df[posts_df['author_id'] == uid]\n",
        "        texts = user_posts['text'].dropna().tolist()\n",
        "\n",
        "        feats = {'user_id': uid}\n",
        "\n",
        "        if len(texts) < 2:\n",
        "            feats.update({\n",
        "                'emb_avg_sim': 0, 'emb_std_sim': 0,\n",
        "                'emb_min_sim': 0, 'emb_max_sim': 0,\n",
        "                'emb_median_sim': 0\n",
        "            })\n",
        "            results.append(feats)\n",
        "            continue\n",
        "\n",
        "        # Sample up to 50 tweets for performance\n",
        "        sample = texts[:50]\n",
        "        embeddings = model.encode(sample, show_progress_bar=False, batch_size=32)\n",
        "\n",
        "        # Store mean embedding for cross-user analysis\n",
        "        mean_embeddings[uid] = np.mean(embeddings, axis=0)\n",
        "\n",
        "        # Pairwise cosine similarities (upper triangle)\n",
        "        sim_matrix = cosine_similarity(embeddings)\n",
        "        upper_tri = sim_matrix[np.triu_indices_from(sim_matrix, k=1)]\n",
        "\n",
        "        feats['emb_avg_sim'] = float(np.mean(upper_tri))\n",
        "        feats['emb_std_sim'] = float(np.std(upper_tri))\n",
        "        feats['emb_min_sim'] = float(np.min(upper_tri))\n",
        "        feats['emb_max_sim'] = float(np.max(upper_tri))\n",
        "        feats['emb_median_sim'] = float(np.median(upper_tri))\n",
        "\n",
        "        results.append(feats)\n",
        "\n",
        "    return pd.DataFrame(results), mean_embeddings\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 3b. GPT-2 Perplexity Features\n",
        "# ============================================================\n",
        "def load_perplexity_model(model_name='gpt2'):\n",
        "    \"\"\"Load GPT-2 model for perplexity computation.\"\"\"\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    tokenizer = GPT2TokenizerFast.from_pretrained(model_name)\n",
        "    model = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
        "    model.eval()\n",
        "    return model, tokenizer, device\n",
        "\n",
        "\n",
        "def compute_perplexity(text, model, tokenizer, device, max_length=512):\n",
        "    \"\"\"Compute perplexity of a text using GPT-2.\"\"\"\n",
        "    try:\n",
        "        inputs = tokenizer(text, return_tensors='pt', truncation=True,\n",
        "                          max_length=max_length).to(device)\n",
        "        if inputs['input_ids'].shape[1] < 2:\n",
        "            return None\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs, labels=inputs['input_ids'])\n",
        "        return float(torch.exp(outputs.loss))\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "\n",
        "def extract_perplexity_features(posts_df, users_df, ppl_model, ppl_tokenizer, ppl_device):\n",
        "    \"\"\"Compute per-user perplexity statistics.\"\"\"\n",
        "    results = []\n",
        "\n",
        "    for idx, (_, user) in enumerate(users_df.iterrows()):\n",
        "        uid = user['id']\n",
        "        user_posts = posts_df[posts_df['author_id'] == uid]\n",
        "        texts = user_posts['text'].dropna().tolist()\n",
        "\n",
        "        feats = {'user_id': uid}\n",
        "\n",
        "        if not texts:\n",
        "            feats.update({\n",
        "                'ppl_mean': 0, 'ppl_std': 0, 'ppl_min': 0,\n",
        "                'ppl_max': 0, 'ppl_median': 0, 'ppl_skew': 0,\n",
        "                'ppl_low_ratio': 0\n",
        "            })\n",
        "            results.append(feats)\n",
        "            continue\n",
        "\n",
        "        # Sample up to 20 tweets for performance\n",
        "        sample = texts[:20]\n",
        "        perplexities = [compute_perplexity(t, ppl_model, ppl_tokenizer, ppl_device)\n",
        "                       for t in sample]\n",
        "        perplexities = [p for p in perplexities if p is not None and p < 10000]\n",
        "\n",
        "        if perplexities:\n",
        "            feats['ppl_mean'] = np.mean(perplexities)\n",
        "            feats['ppl_std'] = np.std(perplexities)\n",
        "            feats['ppl_min'] = np.min(perplexities)\n",
        "            feats['ppl_max'] = np.max(perplexities)\n",
        "            feats['ppl_median'] = np.median(perplexities)\n",
        "            feats['ppl_skew'] = float(stats.skew(perplexities)) if len(perplexities) >= 3 else 0\n",
        "            # Ratio of tweets with unusually low perplexity (< 50 = very \"fluent\")\n",
        "            feats['ppl_low_ratio'] = np.mean([1 if p < 50 else 0 for p in perplexities])\n",
        "        else:\n",
        "            feats.update({\n",
        "                'ppl_mean': 0, 'ppl_std': 0, 'ppl_min': 0,\n",
        "                'ppl_max': 0, 'ppl_median': 0, 'ppl_skew': 0,\n",
        "                'ppl_low_ratio': 0\n",
        "            })\n",
        "\n",
        "        results.append(feats)\n",
        "\n",
        "        if (idx + 1) % 50 == 0:\n",
        "            print(f\"  Perplexity: {idx+1}/{len(users_df)} users processed\")\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "print(\"NLP feature functions defined.\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "b79cface"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Load models once\n",
        "print(\"Loading Sentence Transformer...\")\n",
        "embed_model = SentenceTransformer(EMBED_MODEL_NAME)\n",
        "print(\"Loading GPT-2 for perplexity...\")\n",
        "ppl_model, ppl_tokenizer, ppl_device = load_perplexity_model('gpt2')\n",
        "print(f\"Models loaded. Perplexity device: {ppl_device}\")\n",
        "\n",
        "# Extract NLP features for all datasets\n",
        "nlp_features = {}\n",
        "all_mean_embeddings = {}\n",
        "\n",
        "for ds_id, ds in datasets.items():\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"Dataset {ds_id} ({ds['meta']['lang']})\")\n",
        "    print(f\"{'='*50}\")\n",
        "\n",
        "    # Embedding features\n",
        "    print(\"Computing sentence embeddings...\")\n",
        "    emb_feats, mean_embs = extract_embedding_features(\n",
        "        ds['posts'], ds['users'], model=embed_model)\n",
        "\n",
        "    # Perplexity features\n",
        "    print(\"Computing perplexity scores...\")\n",
        "    ppl_feats = extract_perplexity_features(\n",
        "        ds['posts'], ds['users'], ppl_model, ppl_tokenizer, ppl_device)\n",
        "\n",
        "    # Merge\n",
        "    nlp_feat = emb_feats.merge(ppl_feats, on='user_id', how='outer')\n",
        "    nlp_features[ds_id] = nlp_feat\n",
        "    all_mean_embeddings[ds_id] = mean_embs\n",
        "\n",
        "    print(f\"  NLP features shape: {nlp_feat.shape}\")\n",
        "\n",
        "print(\"\\nNLP feature extraction complete!\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "7c58aea4"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 4: Cross-User Features (Layer 3) + HDBSCAN Clustering + Combine All\n",
        "\n",
        "Enhanced with:\n",
        "- Original cross-user embedding similarity\n",
        "- **HDBSCAN clustering** on embeddings (users in same cluster as bots are more suspicious)\n",
        "- **Temporal pattern clustering** on hour distributions\n",
        "- Combine ALL feature layers into final feature matrices"
      ],
      "id": "03f212a7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# Layer 3: Cross-user embedding similarity features\n",
        "# ============================================================\n",
        "def extract_cross_user_features(mean_embeddings):\n",
        "    \"\"\"Compute how similar each user's avg embedding is to all other users.\"\"\"\n",
        "    user_ids = list(mean_embeddings.keys())\n",
        "    if len(user_ids) < 2:\n",
        "        return pd.DataFrame(columns=['user_id', 'cross_avg_sim', 'cross_max_sim',\n",
        "                                      'cross_min_sim', 'cross_std_sim'])\n",
        "\n",
        "    emb_matrix = np.array([mean_embeddings[uid] for uid in user_ids])\n",
        "    sim_matrix = cosine_similarity(emb_matrix)\n",
        "\n",
        "    results = []\n",
        "    for i, uid in enumerate(user_ids):\n",
        "        sims = np.delete(sim_matrix[i], i)  # Exclude self-similarity\n",
        "        results.append({\n",
        "            'user_id': uid,\n",
        "            'cross_avg_sim': float(np.mean(sims)),\n",
        "            'cross_max_sim': float(np.max(sims)),\n",
        "            'cross_min_sim': float(np.min(sims)),\n",
        "            'cross_std_sim': float(np.std(sims)),\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Layer 3b: HDBSCAN Clustering Features (NEW)\n",
        "# ============================================================\n",
        "def extract_clustering_features(mean_embeddings, posts_df, users_df):\n",
        "    \"\"\"\n",
        "    Cluster users using HDBSCAN on embeddings and temporal patterns.\n",
        "    Returns per-user cluster membership and cluster-level statistics.\n",
        "    \"\"\"\n",
        "    user_ids = list(mean_embeddings.keys())\n",
        "    results = {uid: {} for uid in users_df['id'].tolist()}\n",
        "\n",
        "    # --- Embedding-based HDBSCAN Clustering ---\n",
        "    if len(user_ids) >= 10:\n",
        "        emb_matrix = np.array([mean_embeddings[uid] for uid in user_ids])\n",
        "\n",
        "        # Normalize for better clustering\n",
        "        scaler = StandardScaler()\n",
        "        emb_scaled = scaler.fit_transform(emb_matrix)\n",
        "\n",
        "        clusterer = hdbscan.HDBSCAN(min_cluster_size=3, min_samples=2, metric='euclidean')\n",
        "        cluster_labels = clusterer.fit_predict(emb_scaled)\n",
        "\n",
        "        for i, uid in enumerate(user_ids):\n",
        "            results[uid]['emb_cluster_id'] = int(cluster_labels[i])\n",
        "            results[uid]['emb_cluster_prob'] = float(clusterer.probabilities_[i])\n",
        "            # Cluster size (how many users in same cluster)\n",
        "            if cluster_labels[i] >= 0:\n",
        "                results[uid]['emb_cluster_size'] = int((cluster_labels == cluster_labels[i]).sum())\n",
        "            else:\n",
        "                results[uid]['emb_cluster_size'] = 0  # Noise point\n",
        "            results[uid]['emb_is_noise'] = int(cluster_labels[i] == -1)\n",
        "    else:\n",
        "        for uid in user_ids:\n",
        "            results[uid]['emb_cluster_id'] = -1\n",
        "            results[uid]['emb_cluster_prob'] = 0.0\n",
        "            results[uid]['emb_cluster_size'] = 0\n",
        "            results[uid]['emb_is_noise'] = 1\n",
        "\n",
        "    # --- Temporal Pattern Clustering ---\n",
        "    # Build 24-dim hour distribution vector per user\n",
        "    temporal_vectors = {}\n",
        "    for _, user in users_df.iterrows():\n",
        "        uid = user['id']\n",
        "        user_posts_local = posts_df[posts_df['author_id'] == uid]\n",
        "        if len(user_posts_local) >= 3:\n",
        "            try:\n",
        "                ts = pd.to_datetime(user_posts_local['created_at'])\n",
        "                hour_dist = np.zeros(24)\n",
        "                for h in ts.dt.hour:\n",
        "                    hour_dist[h] += 1\n",
        "                hour_dist /= max(hour_dist.sum(), 1)\n",
        "                temporal_vectors[uid] = hour_dist\n",
        "            except:\n",
        "                temporal_vectors[uid] = np.zeros(24)\n",
        "        else:\n",
        "            temporal_vectors[uid] = np.zeros(24)\n",
        "\n",
        "    temp_user_ids = list(temporal_vectors.keys())\n",
        "    if len(temp_user_ids) >= 10:\n",
        "        temp_matrix = np.array([temporal_vectors[uid] for uid in temp_user_ids])\n",
        "\n",
        "        temp_clusterer = hdbscan.HDBSCAN(min_cluster_size=3, min_samples=2, metric='euclidean')\n",
        "        temp_labels = temp_clusterer.fit_predict(temp_matrix)\n",
        "\n",
        "        for i, uid in enumerate(temp_user_ids):\n",
        "            results[uid]['temp_cluster_id'] = int(temp_labels[i])\n",
        "            results[uid]['temp_cluster_prob'] = float(temp_clusterer.probabilities_[i])\n",
        "            if temp_labels[i] >= 0:\n",
        "                results[uid]['temp_cluster_size'] = int((temp_labels == temp_labels[i]).sum())\n",
        "            else:\n",
        "                results[uid]['temp_cluster_size'] = 0\n",
        "            results[uid]['temp_is_noise'] = int(temp_labels[i] == -1)\n",
        "    else:\n",
        "        for uid in temp_user_ids:\n",
        "            results[uid]['temp_cluster_id'] = -1\n",
        "            results[uid]['temp_cluster_prob'] = 0.0\n",
        "            results[uid]['temp_cluster_size'] = 0\n",
        "            results[uid]['temp_is_noise'] = 1\n",
        "\n",
        "    # Fill defaults for any users missing features\n",
        "    default_cluster = {\n",
        "        'emb_cluster_id': -1, 'emb_cluster_prob': 0.0, 'emb_cluster_size': 0, 'emb_is_noise': 1,\n",
        "        'temp_cluster_id': -1, 'temp_cluster_prob': 0.0, 'temp_cluster_size': 0, 'temp_is_noise': 1,\n",
        "    }\n",
        "    rows = []\n",
        "    for uid in users_df['id'].tolist():\n",
        "        row = {'user_id': uid}\n",
        "        for k, v in default_cluster.items():\n",
        "            row[k] = results.get(uid, {}).get(k, v)\n",
        "        rows.append(row)\n",
        "\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Combine ALL features into final feature matrices\n",
        "# ============================================================\n",
        "feature_dfs = {}\n",
        "for ds_id in datasets.keys():\n",
        "    print(f\"\\nCombining features for dataset {ds_id}...\")\n",
        "\n",
        "    # Start with basic features\n",
        "    df = basic_features[ds_id].copy()\n",
        "\n",
        "    # Merge advanced temporal features (NEW)\n",
        "    if ds_id in advanced_temporal_features:\n",
        "        df = df.merge(advanced_temporal_features[ds_id], on='user_id', how='left')\n",
        "\n",
        "    # Merge stylometry features (NEW)\n",
        "    if ds_id in stylometry_features:\n",
        "        df = df.merge(stylometry_features[ds_id], on='user_id', how='left')\n",
        "\n",
        "    # Merge NLP features\n",
        "    df = df.merge(nlp_features[ds_id], on='user_id', how='left')\n",
        "\n",
        "    # Cross-user features\n",
        "    if ds_id in all_mean_embeddings and all_mean_embeddings[ds_id]:\n",
        "        cross_feats = extract_cross_user_features(all_mean_embeddings[ds_id])\n",
        "        df = df.merge(cross_feats, on='user_id', how='left')\n",
        "\n",
        "    # HDBSCAN clustering features (NEW)\n",
        "    if ds_id in all_mean_embeddings and all_mean_embeddings[ds_id]:\n",
        "        cluster_feats = extract_clustering_features(\n",
        "            all_mean_embeddings[ds_id], datasets[ds_id]['posts'], datasets[ds_id]['users'])\n",
        "        df = df.merge(cluster_feats, on='user_id', how='left')\n",
        "\n",
        "    # Fill NaN\n",
        "    df = df.fillna(0)\n",
        "    feature_dfs[ds_id] = df\n",
        "    print(f\"  Final feature shape: {df.shape}\")\n",
        "\n",
        "# List all feature columns (exclude metadata columns)\n",
        "meta_cols = ['user_id', 'is_bot']\n",
        "feature_cols = [c for c in feature_dfs[30].columns if c not in meta_cols]\n",
        "print(f\"\\nTotal features: {len(feature_cols)}\")\n",
        "print(f\"Features: {feature_cols}\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "c65d075c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 5: Model Training & Stacking Ensemble (Enhanced)\n",
        "\n",
        "Enhanced with:\n",
        "- **Tuned scale_pos_weight**: Adjusted based on competition scoring (+4 TP, -1 FN, -2 FP)\n",
        "- **Stacking Ensemble**: Layer 1 (XGBoost + LightGBM + CatBoost) -> Layer 2 (LogisticRegression)\n",
        "- **Fine-grained threshold search**: Two-pass coarse-to-fine search for optimal threshold"
      ],
      "id": "868ef184"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# Competition Scoring Function\n",
        "# ============================================================\n",
        "def competition_score(y_true, y_pred):\n",
        "    \"\"\"Calculate the competition score: +4 TP, -1 FN, -2 FP.\"\"\"\n",
        "    y_true = np.array(y_true)\n",
        "    y_pred = np.array(y_pred)\n",
        "    tp = ((y_pred == 1) & (y_true == 1)).sum()\n",
        "    fn = ((y_pred == 0) & (y_true == 1)).sum()\n",
        "    fp = ((y_pred == 1) & (y_true == 0)).sum()\n",
        "    tn = ((y_pred == 0) & (y_true == 0)).sum()\n",
        "    score = 4 * tp - 1 * fn - 2 * fp\n",
        "    return score, {'tp': int(tp), 'fn': int(fn), 'fp': int(fp), 'tn': int(tn)}\n",
        "\n",
        "\n",
        "def find_optimal_threshold(y_true, y_proba):\n",
        "    \"\"\"\n",
        "    Two-pass coarse-to-fine threshold search for competition score maximization.\n",
        "    Pass 1: coarse search (step=0.05) to find approximate region\n",
        "    Pass 2: fine search (step=0.002) around the best region\n",
        "    \"\"\"\n",
        "    best_score = -np.inf\n",
        "    best_threshold = 0.5\n",
        "    best_details = {}\n",
        "\n",
        "    # Pass 1: Coarse search\n",
        "    for threshold in np.arange(0.05, 0.96, 0.05):\n",
        "        y_pred = (y_proba >= threshold).astype(int)\n",
        "        score, details = competition_score(y_true, y_pred)\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best_threshold = threshold\n",
        "            best_details = details\n",
        "\n",
        "    # Pass 2: Fine search around the best threshold from pass 1\n",
        "    fine_start = max(0.01, best_threshold - 0.08)\n",
        "    fine_end = min(0.99, best_threshold + 0.08)\n",
        "    for threshold in np.arange(fine_start, fine_end, 0.002):\n",
        "        y_pred = (y_proba >= threshold).astype(int)\n",
        "        score, details = competition_score(y_true, y_pred)\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best_threshold = threshold\n",
        "            best_details = details\n",
        "\n",
        "    return best_threshold, best_score, best_details\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Custom Asymmetric Loss for Competition Scoring (CORE CHANGE)\n",
        "# ============================================================\n",
        "# Competition scoring: +4 TP, -1 FN, -2 FP, 0 TN\n",
        "#\n",
        "# Cost analysis (from the model's perspective):\n",
        "#   - Missing a bot (FN): lose +4 TP reward AND get -1 FN penalty = 5 point swing\n",
        "#   - False alarm  (FP): get -2 FP penalty                       = 2 point swing\n",
        "#   - Ratio: 5:2 = 2.5x bias towards catching bots (recall)\n",
        "#\n",
        "# Instead of using scale_pos_weight (indirect), we directly encode\n",
        "# these asymmetric costs into the gradient/hessian of the loss function.\n",
        "# This makes the model learn the EXACT cost structure during training.\n",
        "\n",
        "W_POS = 5.0  # Weight for bot class (positive): TP value (4) + FN cost (1)\n",
        "W_NEG = 2.0  # Weight for human class (negative): FP cost (2)\n",
        "\n",
        "\n",
        "def _safe_sigmoid(x):\n",
        "    \"\"\"Numerically stable sigmoid function.\"\"\"\n",
        "    return 1.0 / (1.0 + np.exp(-np.clip(np.asarray(x, dtype=np.float64), -500, 500)))\n",
        "\n",
        "\n",
        "def competition_loss_xgb(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Custom asymmetric weighted cross-entropy for XGBoost (sklearn API).\n",
        "\n",
        "    Directly encodes the competition's asymmetric scoring into the loss:\n",
        "    - Bot samples (y=1) get weight W_POS=5 (high cost of missing)\n",
        "    - Human samples (y=0) get weight W_NEG=2 (cost of false alarm)\n",
        "\n",
        "    Args:\n",
        "        y_true: true labels (0 or 1)\n",
        "        y_pred: raw predictions (logits, before sigmoid)\n",
        "    Returns:\n",
        "        (gradient, hessian) tuple\n",
        "    \"\"\"\n",
        "    p = _safe_sigmoid(y_pred)\n",
        "    w = np.where(y_true == 1, W_POS, W_NEG)\n",
        "    # Gradient of weighted binary cross-entropy w.r.t. logit\n",
        "    grad = w * (p - y_true)\n",
        "    # Hessian of weighted binary cross-entropy w.r.t. logit\n",
        "    hess = w * np.maximum(p * (1.0 - p), 1e-7)\n",
        "    return grad, hess\n",
        "\n",
        "\n",
        "def competition_loss_lgb(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Custom asymmetric weighted cross-entropy for LightGBM (sklearn API).\n",
        "    Same math as XGBoost version, separate function for clarity.\n",
        "    \"\"\"\n",
        "    p = _safe_sigmoid(y_pred)\n",
        "    w = np.where(y_true == 1, W_POS, W_NEG)\n",
        "    grad = w * (p - y_true)\n",
        "    hess = w * np.maximum(p * (1.0 - p), 1e-7)\n",
        "    return grad, hess\n",
        "\n",
        "\n",
        "def train_models(X_train, y_train, params=None):\n",
        "    \"\"\"\n",
        "    Train XGBoost, LightGBM, and CatBoost with CUSTOM ASYMMETRIC LOSS.\n",
        "    Layer 1 of the stacking ensemble.\n",
        "\n",
        "    Key change: replaced scale_pos_weight with custom objective functions\n",
        "    that directly encode the competition's asymmetric scoring (4*TP - FN - 2*FP).\n",
        "\n",
        "    Args:\n",
        "        X_train: training features\n",
        "        y_train: training labels\n",
        "        params: optional dict of hyperparameters from Optuna optimization.\n",
        "                If None, uses default hyperparameters.\n",
        "    \"\"\"\n",
        "    # Default hyperparameters (used when params is None)\n",
        "    if params is None:\n",
        "        params = {\n",
        "            'n_estimators': 600,\n",
        "            'max_depth': 5,\n",
        "            'learning_rate': 0.025,\n",
        "            'subsample': 0.8,\n",
        "            'colsample_bytree': 0.7,\n",
        "            'min_child_weight': 3,\n",
        "            'reg_alpha': 0.1,\n",
        "            'reg_lambda': 1.0,\n",
        "            'gamma': 0.1,\n",
        "        }\n",
        "\n",
        "    # XGBoost with custom asymmetric loss\n",
        "    # NOTE: removed scale_pos_weight and eval_metric='logloss'\n",
        "    #       because the custom objective handles asymmetric costs directly\n",
        "    xgb_model = xgb.XGBClassifier(\n",
        "        objective=competition_loss_xgb,\n",
        "        n_estimators=params['n_estimators'],\n",
        "        max_depth=params['max_depth'],\n",
        "        learning_rate=params['learning_rate'],\n",
        "        subsample=params['subsample'],\n",
        "        colsample_bytree=params['colsample_bytree'],\n",
        "        min_child_weight=params['min_child_weight'],\n",
        "        reg_alpha=params['reg_alpha'],\n",
        "        reg_lambda=params['reg_lambda'],\n",
        "        gamma=params['gamma'],\n",
        "        random_state=42, verbosity=0\n",
        "    )\n",
        "    xgb_model.fit(X_train, y_train)\n",
        "\n",
        "    # LightGBM with custom asymmetric loss\n",
        "    lgb_model = lgb.LGBMClassifier(\n",
        "        objective=competition_loss_lgb,\n",
        "        n_estimators=params['n_estimators'],\n",
        "        max_depth=params['max_depth'],\n",
        "        learning_rate=params['learning_rate'],\n",
        "        subsample=params['subsample'],\n",
        "        colsample_bytree=params['colsample_bytree'],\n",
        "        min_child_weight=params['min_child_weight'],\n",
        "        reg_alpha=params['reg_alpha'],\n",
        "        reg_lambda=params['reg_lambda'],\n",
        "        random_state=42, verbose=-1\n",
        "    )\n",
        "    lgb_model.fit(X_train, y_train)\n",
        "\n",
        "    # CatBoost with equivalent asymmetric class weights\n",
        "    # CatBoost doesn't support custom obj as easily via sklearn API,\n",
        "    # so we use class_weights with the same 5:2 ratio\n",
        "    cb_model = CatBoostClassifier(\n",
        "        iterations=params['n_estimators'],\n",
        "        depth=min(params['max_depth'], 10),  # CatBoost max depth capped at 10\n",
        "        learning_rate=params['learning_rate'],\n",
        "        class_weights={0: 1.0, 1: W_POS / W_NEG},  # {0: 1.0, 1: 2.5}\n",
        "        random_seed=42, verbose=0\n",
        "    )\n",
        "    cb_model.fit(X_train, y_train)\n",
        "\n",
        "    return {'xgb': xgb_model, 'lgb': lgb_model, 'cb': cb_model}\n",
        "\n",
        "\n",
        "def get_base_model_probas(models, X):\n",
        "    \"\"\"\n",
        "    Get probability predictions from each base model (for stacking).\n",
        "\n",
        "    IMPORTANT: With custom objectives, XGBoost/LightGBM's predict_proba()\n",
        "    returns RAW LOGITS instead of probabilities (they don't know the link\n",
        "    function for custom objectives). We extract raw predictions and apply\n",
        "    sigmoid manually.\n",
        "    CatBoost with class_weights uses standard objective, so predict_proba works.\n",
        "    \"\"\"\n",
        "    probas = {}\n",
        "    for name, model in models.items():\n",
        "        if name == 'xgb':\n",
        "            # XGBoost: get raw margin via booster and apply sigmoid\n",
        "            dmat = xgb.DMatrix(X)\n",
        "            raw = model.get_booster().predict(dmat, output_margin=True)\n",
        "            probas[name] = _safe_sigmoid(raw)\n",
        "        elif name == 'lgb':\n",
        "            # LightGBM: get raw score and apply sigmoid\n",
        "            raw = model.predict(X, raw_score=True)\n",
        "            probas[name] = _safe_sigmoid(raw)\n",
        "        else:\n",
        "            # CatBoost: predict_proba works correctly with class_weights\n",
        "            probas[name] = model.predict_proba(X)[:, 1]\n",
        "    return probas\n",
        "\n",
        "\n",
        "def train_stacking_meta(models, X_train, y_train):\n",
        "    \"\"\"\n",
        "    Train Layer 2 meta-learner using cross-validated predictions from Layer 1.\n",
        "    Uses LogisticRegression as the meta-learner to learn optimal model weighting.\n",
        "    \"\"\"\n",
        "    from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "    n_models = len(models)\n",
        "    meta_features = np.zeros((len(X_train), n_models))\n",
        "\n",
        "    # Generate out-of-fold predictions for meta-training\n",
        "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "    for fold_idx, (tr_idx, val_idx) in enumerate(skf.split(X_train, y_train)):\n",
        "        X_tr, X_val = X_train[tr_idx], X_train[val_idx]\n",
        "        y_tr = y_train[tr_idx]\n",
        "\n",
        "        # Train fold models\n",
        "        fold_models = train_models(X_tr, y_tr)\n",
        "\n",
        "        # Get OOF predictions (using get_base_model_probas for proper sigmoid handling)\n",
        "        fold_probas = get_base_model_probas(fold_models, X_val)\n",
        "        for i, name in enumerate(fold_models.keys()):\n",
        "            meta_features[val_idx, i] = fold_probas[name]\n",
        "\n",
        "    # Train meta-learner on OOF predictions\n",
        "    meta_model = LogisticRegressionCV(\n",
        "        Cs=10, cv=3, scoring='roc_auc',\n",
        "        class_weight='balanced', random_state=42, max_iter=1000\n",
        "    )\n",
        "    meta_model.fit(meta_features, y_train)\n",
        "\n",
        "    print(f\"  Stacking meta-learner trained. Coefficients: {dict(zip(models.keys(), meta_model.coef_[0]))}\")\n",
        "    return meta_model\n",
        "\n",
        "\n",
        "def ensemble_predict_proba(models, X, meta_model=None):\n",
        "    \"\"\"\n",
        "    Get ensemble probability.\n",
        "    If meta_model is provided: use stacking (Layer 2 meta-learner).\n",
        "    Otherwise: fall back to simple average.\n",
        "    \"\"\"\n",
        "    base_probas = get_base_model_probas(models, X)\n",
        "    probas_array = np.column_stack(list(base_probas.values()))\n",
        "\n",
        "    if meta_model is not None:\n",
        "        return meta_model.predict_proba(probas_array)[:, 1]\n",
        "    else:\n",
        "        return np.mean(probas_array, axis=1)\n",
        "\n",
        "\n",
        "def evaluate_on_test(models, X_test, y_test, dataset_name=\"\", meta_model=None):\n",
        "    \"\"\"Evaluate ensemble on test set with optimal threshold.\"\"\"\n",
        "    proba = ensemble_predict_proba(models, X_test, meta_model=meta_model)\n",
        "    threshold, score, details = find_optimal_threshold(y_test, proba)\n",
        "\n",
        "    y_pred = (proba >= threshold).astype(int)\n",
        "\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"Evaluation: {dataset_name}\")\n",
        "    print(f\"{'='*50}\")\n",
        "    print(f\"Optimal threshold: {threshold:.2f}\")\n",
        "    print(f\"Competition score: {score}\")\n",
        "    print(f\"Details: TP={details['tp']}, FN={details['fn']}, FP={details['fp']}, TN={details['tn']}\")\n",
        "    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "    print(classification_report(y_test, y_pred, target_names=['Human', 'Bot']))\n",
        "\n",
        "    return threshold, score, proba\n",
        "\n",
        "print(\"Training functions defined.\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "fe49a205"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 6: Cross-Validation Between Datasets\n",
        "\n",
        "- English: Train on 30 -> Test on 32, then Train on 32 -> Test on 30\n",
        "- French: Train on 31 -> Test on 33, then Train on 33 -> Test on 31"
      ],
      "id": "6549534c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# Optuna Bayesian Hyperparameter Optimization\n",
        "# ============================================================\n",
        "# Replaces manual grid/random search with efficient TPE (Tree-structured\n",
        "# Parzen Estimator) sampler. The competition score (+4 TP, -1 FN, -2 FP)\n",
        "# is used directly as the optimization objective.\n",
        "\n",
        "def optuna_optimize_hyperparams(X_train, y_train, n_trials=200, n_inner_folds=3):\n",
        "    \"\"\"\n",
        "    Use Optuna Bayesian optimization (TPE) to find the best hyperparameters\n",
        "    for the XGBoost + LightGBM + CatBoost stacking ensemble.\n",
        "\n",
        "    The objective maximizes the average competition score across inner CV folds.\n",
        "\n",
        "    Args:\n",
        "        X_train: training feature matrix (numpy array)\n",
        "        y_train: training labels (numpy array)\n",
        "        n_trials: number of Optuna trials (default 200, increase for better results)\n",
        "        n_inner_folds: number of inner CV folds for evaluation (default 3)\n",
        "\n",
        "    Returns:\n",
        "        optuna.Study object (access best_params, best_value, etc.)\n",
        "    \"\"\"\n",
        "\n",
        "    def objective(trial):\n",
        "        params = {\n",
        "            'n_estimators': trial.suggest_int('n_estimators', 100, 2000),\n",
        "            'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
        "            'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.3, log=True),\n",
        "            'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
        "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.3, 1.0),\n",
        "            'min_child_weight': trial.suggest_int('min_child_weight', 1, 20),\n",
        "            'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),\n",
        "            'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n",
        "            'gamma': trial.suggest_float('gamma', 1e-8, 5.0, log=True),\n",
        "        }\n",
        "\n",
        "        # Inner cross-validation to evaluate this set of hyperparameters\n",
        "        skf = StratifiedKFold(n_splits=n_inner_folds, shuffle=True, random_state=42)\n",
        "        fold_scores = []\n",
        "\n",
        "        for fold_idx, (tr_idx, val_idx) in enumerate(skf.split(X_train, y_train)):\n",
        "            X_tr, X_val = X_train[tr_idx], X_train[val_idx]\n",
        "            y_tr, y_val = y_train[tr_idx], y_train[val_idx]\n",
        "\n",
        "            # Train all base models with the suggested hyperparameters\n",
        "            fold_models = train_models(X_tr, y_tr, params=params)\n",
        "\n",
        "            # Get ensemble probability (simple average, no meta-learner in inner loop\n",
        "            # for speed -- the meta-learner will be trained after optimization)\n",
        "            probas = get_base_model_probas(fold_models, X_val)\n",
        "            avg_proba = np.mean(np.column_stack(list(probas.values())), axis=1)\n",
        "\n",
        "            # Evaluate with competition score using optimal threshold\n",
        "            _, score, _ = find_optimal_threshold(y_val, avg_proba)\n",
        "            fold_scores.append(score)\n",
        "\n",
        "        return np.mean(fold_scores)\n",
        "\n",
        "    # Create Optuna study with TPE sampler (Bayesian optimization)\n",
        "    study = optuna.create_study(\n",
        "        direction='maximize',\n",
        "        sampler=optuna.samplers.TPESampler(seed=42),\n",
        "        pruner=optuna.pruners.MedianPruner(n_startup_trials=10)\n",
        "    )\n",
        "    study.optimize(\n",
        "        objective,\n",
        "        n_trials=n_trials,\n",
        "        show_progress_bar=True,\n",
        "        n_jobs=1  # set to -1 for parallel trials if resources allow\n",
        "    )\n",
        "\n",
        "    # Print optimization results\n",
        "    print(f\"\\n  Optuna optimization complete ({n_trials} trials)\")\n",
        "    print(f\"  Best competition score (inner CV avg): {study.best_value:.2f}\")\n",
        "    print(f\"  Best hyperparameters:\")\n",
        "    for key, value in study.best_params.items():\n",
        "        if isinstance(value, float):\n",
        "            print(f\"    {key}: {value:.6f}\")\n",
        "        else:\n",
        "            print(f\"    {key}: {value}\")\n",
        "\n",
        "    return study\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Save / Load Hyperparameters (JSON)\n",
        "# ============================================================\n",
        "# On Google Colab, saves to Google Drive so hyperparameters persist\n",
        "# across sessions. Locally, saves to saved_hyperparams/ in the\n",
        "# current working directory.\n",
        "# ============================================================\n",
        "import os\n",
        "\n",
        "# Auto-detect Colab and use Google Drive for persistent storage\n",
        "IN_COLAB = 'google.colab' in str(get_ipython()) if 'get_ipython' in dir() else False\n",
        "\n",
        "if IN_COLAB:\n",
        "    from google.colab import drive\n",
        "    if not os.path.ismount('/content/drive'):\n",
        "        drive.mount('/content/drive')\n",
        "    HYPERPARAMS_DIR = \"/content/drive/MyDrive/bot_or_not/saved_hyperparams\"\n",
        "    print(f\"[Colab] Hyperparameters will be saved to Google Drive: {HYPERPARAMS_DIR}\")\n",
        "else:\n",
        "    HYPERPARAMS_DIR = \"saved_hyperparams\"\n",
        "    print(f\"[Local] Hyperparameters will be saved to: {HYPERPARAMS_DIR}\")\n",
        "\n",
        "def save_hyperparams(params, name, score=None, directory=HYPERPARAMS_DIR):\n",
        "    \"\"\"\n",
        "    Save hyperparameters to a JSON file for later reuse.\n",
        "\n",
        "    Args:\n",
        "        params: dict of hyperparameters\n",
        "        name: identifier string, e.g. 'en_30_32' or 'best_english'\n",
        "        score: optional competition score to save alongside\n",
        "        directory: folder to save into (default: saved_hyperparams/)\n",
        "    \"\"\"\n",
        "    os.makedirs(directory, exist_ok=True)\n",
        "    filepath = os.path.join(directory, f\"{name}.json\")\n",
        "    payload = {\n",
        "        \"hyperparameters\": params,\n",
        "        \"score\": score,\n",
        "        \"saved_at\": pd.Timestamp.now().isoformat(),\n",
        "    }\n",
        "    with open(filepath, 'w') as f:\n",
        "        json.dump(payload, f, indent=2, default=str)\n",
        "    print(f\"  [SAVED] Hyperparameters -> {filepath}\")\n",
        "    return filepath\n",
        "\n",
        "\n",
        "def load_hyperparams(name, directory=HYPERPARAMS_DIR):\n",
        "    \"\"\"\n",
        "    Load previously saved hyperparameters from a JSON file.\n",
        "\n",
        "    Args:\n",
        "        name: identifier string used when saving\n",
        "        directory: folder to look in (default: saved_hyperparams/)\n",
        "\n",
        "    Returns:\n",
        "        dict of hyperparameters, or None if file not found\n",
        "    \"\"\"\n",
        "    filepath = os.path.join(directory, f\"{name}.json\")\n",
        "    if not os.path.exists(filepath):\n",
        "        print(f\"  [WARNING] No saved hyperparameters found at {filepath}\")\n",
        "        return None\n",
        "    with open(filepath, 'r') as f:\n",
        "        payload = json.load(f)\n",
        "    params = payload[\"hyperparameters\"]\n",
        "    score = payload.get(\"score\")\n",
        "    saved_at = payload.get(\"saved_at\", \"unknown\")\n",
        "    print(f\"  [LOADED] Hyperparameters from {filepath} (score={score}, saved_at={saved_at})\")\n",
        "    # Restore int types for integer hyperparameters\n",
        "    int_keys = ['n_estimators', 'max_depth', 'min_child_weight']\n",
        "    for k in int_keys:\n",
        "        if k in params:\n",
        "            params[k] = int(params[k])\n",
        "    return params\n",
        "\n",
        "\n",
        "def list_saved_hyperparams(directory=HYPERPARAMS_DIR):\n",
        "    \"\"\"List all saved hyperparameter files.\"\"\"\n",
        "    if not os.path.exists(directory):\n",
        "        print(\"No saved hyperparameters directory found.\")\n",
        "        return []\n",
        "    files = [f for f in os.listdir(directory) if f.endswith('.json')]\n",
        "    if not files:\n",
        "        print(\"No saved hyperparameters found.\")\n",
        "        return []\n",
        "    print(f\"Saved hyperparameters in '{directory}/':\")\n",
        "    for f in sorted(files):\n",
        "        filepath = os.path.join(directory, f)\n",
        "        with open(filepath, 'r') as fh:\n",
        "            payload = json.load(fh)\n",
        "        score = payload.get(\"score\", \"N/A\")\n",
        "        saved_at = payload.get(\"saved_at\", \"unknown\")\n",
        "        print(f\"  - {f}: score={score}, saved_at={saved_at}\")\n",
        "    return files\n",
        "\n",
        "\n",
        "print(\"Optuna hyperparameter optimization function defined.\")\n",
        "print(\"Hyperparameter save/load utilities defined.\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "37cac409"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cross-validation pairs: (train_id, test_id, language)\n",
        "cv_pairs = [\n",
        "    (30, 32, 'English'),\n",
        "    (32, 30, 'English'),\n",
        "    (31, 33, 'French'),\n",
        "    (33, 31, 'French'),\n",
        "]\n",
        "\n",
        "# ============================================================\n",
        "# Optuna Configuration\n",
        "# ============================================================\n",
        "USE_OPTUNA = True          # Set to False to use default hyperparameters\n",
        "OPTUNA_N_TRIALS = 200      # Number of Bayesian optimization trials (increase for better results)\n",
        "OPTUNA_INNER_FOLDS = 3     # Number of inner CV folds for Optuna evaluation\n",
        "\n",
        "# >>> NEW: Set to True to load previously saved hyperparams instead of re-running Optuna <<<\n",
        "LOAD_SAVED_HYPERPARAMS = False  # Set to True to skip Optuna and load from saved_hyperparams/\n",
        "\n",
        "cv_results = {}\n",
        "trained_models = {}\n",
        "trained_meta_models = {}\n",
        "optuna_studies = {}  # Store Optuna studies for analysis\n",
        "\n",
        "for train_id, test_id, lang in cv_pairs:\n",
        "    print(f\"\\n{'#'*60}\")\n",
        "    print(f\"Training on Dataset {train_id} -> Testing on Dataset {test_id} ({lang})\")\n",
        "    print(f\"{'#'*60}\")\n",
        "\n",
        "    train_df = feature_dfs[train_id]\n",
        "    test_df = feature_dfs[test_id]\n",
        "\n",
        "    # Ensure consistent feature columns\n",
        "    common_cols = [c for c in feature_cols if c in train_df.columns and c in test_df.columns]\n",
        "\n",
        "    X_train = train_df[common_cols].values\n",
        "    y_train = train_df['is_bot'].values\n",
        "    X_test = test_df[common_cols].values\n",
        "    y_test = test_df['is_bot'].values\n",
        "\n",
        "    # ---- Hyperparameter Selection ----\n",
        "    best_params = None\n",
        "    param_name = f\"cv_{train_id}_{test_id}\"\n",
        "\n",
        "    if LOAD_SAVED_HYPERPARAMS:\n",
        "        # Try to load saved hyperparameters\n",
        "        best_params = load_hyperparams(param_name)\n",
        "        if best_params is None:\n",
        "            print(f\"  No saved params for {param_name}, falling back to Optuna...\")\n",
        "            LOAD_SAVED_HYPERPARAMS_FALLBACK = True\n",
        "        else:\n",
        "            LOAD_SAVED_HYPERPARAMS_FALLBACK = False\n",
        "    else:\n",
        "        LOAD_SAVED_HYPERPARAMS_FALLBACK = False\n",
        "\n",
        "    if best_params is None and USE_OPTUNA and not LOAD_SAVED_HYPERPARAMS:\n",
        "        # Run Optuna optimization\n",
        "        print(f\"  Running Optuna hyperparameter optimization ({OPTUNA_N_TRIALS} trials)...\")\n",
        "        study = optuna_optimize_hyperparams(\n",
        "            X_train, y_train,\n",
        "            n_trials=OPTUNA_N_TRIALS,\n",
        "            n_inner_folds=OPTUNA_INNER_FOLDS\n",
        "        )\n",
        "        best_params = study.best_params\n",
        "        optuna_studies[(train_id, test_id)] = study\n",
        "        # Auto-save hyperparameters after Optuna optimization\n",
        "        save_hyperparams(best_params, param_name, score=study.best_value)\n",
        "    elif best_params is None and LOAD_SAVED_HYPERPARAMS_FALLBACK:\n",
        "        # Fallback: run Optuna if saved params not found\n",
        "        print(f\"  Running Optuna hyperparameter optimization ({OPTUNA_N_TRIALS} trials)...\")\n",
        "        study = optuna_optimize_hyperparams(\n",
        "            X_train, y_train,\n",
        "            n_trials=OPTUNA_N_TRIALS,\n",
        "            n_inner_folds=OPTUNA_INNER_FOLDS\n",
        "        )\n",
        "        best_params = study.best_params\n",
        "        optuna_studies[(train_id, test_id)] = study\n",
        "        save_hyperparams(best_params, param_name, score=study.best_value)\n",
        "\n",
        "    # Train Layer 1 base models (with Optuna-optimized or default hyperparameters)\n",
        "    print(\"  Training Layer 1 base models...\")\n",
        "    models = train_models(X_train, y_train, params=best_params)\n",
        "    trained_models[(train_id, test_id)] = models\n",
        "\n",
        "    # Train Layer 2 meta-learner (stacking)\n",
        "    print(\"  Training Layer 2 meta-learner (stacking)...\")\n",
        "    meta_model = train_stacking_meta(models, X_train, y_train)\n",
        "    trained_meta_models[(train_id, test_id)] = meta_model\n",
        "\n",
        "    # Evaluate with stacking\n",
        "    threshold, score, proba = evaluate_on_test(\n",
        "        models, X_test, y_test, f\"Train {train_id} -> Test {test_id} ({lang})\",\n",
        "        meta_model=meta_model)\n",
        "\n",
        "    cv_results[(train_id, test_id)] = {\n",
        "        'threshold': threshold, 'score': score,\n",
        "        'lang': lang, 'proba': proba\n",
        "    }\n",
        "\n",
        "# Summary\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"CROSS-VALIDATION SUMMARY (with Stacking Ensemble)\")\n",
        "print(f\"{'='*60}\")\n",
        "for (tr, te), res in cv_results.items():\n",
        "    print(f\"  Train {tr} -> Test {te} ({res['lang']}): \"\n",
        "          f\"Score = {res['score']}, Threshold = {res['threshold']:.2f}\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "2d89695e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Feature importance analysis (using the first English model)\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "first_key = list(trained_models.keys())[0]\n",
        "xgb_model = trained_models[first_key]['xgb']\n",
        "common_cols = [c for c in feature_cols if c in feature_dfs[first_key[0]].columns]\n",
        "\n",
        "importance = pd.DataFrame({\n",
        "    'feature': common_cols,\n",
        "    'importance': xgb_model.feature_importances_\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "plt.figure(figsize=(12, 10))\n",
        "plt.barh(importance['feature'][:25], importance['importance'][:25])\n",
        "plt.xlabel('Feature Importance')\n",
        "plt.title('Top 25 Features (XGBoost)')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nTop 15 features:\")\n",
        "print(importance.head(15).to_string(index=False))"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "275df6cd"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 7: Train Final Models & Build Submission Pipeline\n",
        "\n",
        "Train on ALL available practice data per language, then use for the final evaluation."
      ],
      "id": "0a1fc626"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# Train final models on ALL practice data per language (with Stacking)\n",
        "# ============================================================\n",
        "# Compute average optimal threshold from cross-validation\n",
        "en_thresholds = [v['threshold'] for (tr, te), v in cv_results.items() if v['lang'] == 'English']\n",
        "fr_thresholds = [v['threshold'] for (tr, te), v in cv_results.items() if v['lang'] == 'French']\n",
        "final_en_threshold = np.mean(en_thresholds)\n",
        "final_fr_threshold = np.mean(fr_thresholds)\n",
        "\n",
        "print(f\"Final English threshold: {final_en_threshold:.2f}\")\n",
        "print(f\"Final French threshold: {final_fr_threshold:.2f}\")\n",
        "\n",
        "# Combine English datasets (30 + 32)\n",
        "en_train = pd.concat([feature_dfs[30], feature_dfs[32]], ignore_index=True)\n",
        "common_cols_en = [c for c in feature_cols if c in en_train.columns]\n",
        "X_en = en_train[common_cols_en].values\n",
        "y_en = en_train['is_bot'].values\n",
        "\n",
        "# Select best Optuna params for each language\n",
        "# Priority: 1) Load from saved files  2) From Optuna studies in memory  3) None (use defaults)\n",
        "best_en_params = None\n",
        "best_fr_params = None\n",
        "\n",
        "if LOAD_SAVED_HYPERPARAMS:\n",
        "    # Try to load saved \"best\" params for final models\n",
        "    best_en_params = load_hyperparams(\"best_english\")\n",
        "    best_fr_params = load_hyperparams(\"best_french\")\n",
        "\n",
        "if best_en_params is None and USE_OPTUNA and optuna_studies:\n",
        "    # English: pick params from the CV pair with the best study score\n",
        "    en_studies = {k: v for k, v in optuna_studies.items() if cv_results[k]['lang'] == 'English'}\n",
        "    if en_studies:\n",
        "        best_en_key = max(en_studies, key=lambda k: en_studies[k].best_value)\n",
        "        best_en_params = en_studies[best_en_key].best_params\n",
        "        print(f\"  Using Optuna params from study ({best_en_key}) for final English model\")\n",
        "        # Save the best English params for future reuse\n",
        "        save_hyperparams(best_en_params, \"best_english\", score=en_studies[best_en_key].best_value)\n",
        "\n",
        "if best_fr_params is None and USE_OPTUNA and optuna_studies:\n",
        "    # French: pick params from the CV pair with the best study score\n",
        "    fr_studies = {k: v for k, v in optuna_studies.items() if cv_results[k]['lang'] == 'French'}\n",
        "    if fr_studies:\n",
        "        best_fr_key = max(fr_studies, key=lambda k: fr_studies[k].best_value)\n",
        "        best_fr_params = fr_studies[best_fr_key].best_params\n",
        "        print(f\"  Using Optuna params from study ({best_fr_key}) for final French model\")\n",
        "        # Save the best French params for future reuse\n",
        "        save_hyperparams(best_fr_params, \"best_french\", score=fr_studies[best_fr_key].best_value)\n",
        "\n",
        "print(f\"\\nTraining final English model on {len(X_en)} users ({y_en.sum()} bots)...\")\n",
        "final_en_models = train_models(X_en, y_en, params=best_en_params)\n",
        "print(\"  Training English stacking meta-learner...\")\n",
        "final_en_meta = train_stacking_meta(final_en_models, X_en, y_en)\n",
        "\n",
        "# Combine French datasets (31 + 33)\n",
        "fr_train = pd.concat([feature_dfs[31], feature_dfs[33]], ignore_index=True)\n",
        "common_cols_fr = [c for c in feature_cols if c in fr_train.columns]\n",
        "X_fr = fr_train[common_cols_fr].values\n",
        "y_fr = fr_train['is_bot'].values\n",
        "\n",
        "print(f\"Training final French model on {len(X_fr)} users ({y_fr.sum()} bots)...\")\n",
        "final_fr_models = train_models(X_fr, y_fr, params=best_fr_params)\n",
        "print(\"  Training French stacking meta-learner...\")\n",
        "final_fr_meta = train_stacking_meta(final_fr_models, X_fr, y_fr)\n",
        "\n",
        "print(\"\\nFinal models (with stacking) trained!\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "74136bc0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# FINAL PIPELINE: Process a new dataset and output bot IDs (Enhanced)\n",
        "# ============================================================\n",
        "def detect_bots(json_path, models, threshold, feature_columns,\n",
        "                embed_model, ppl_model, ppl_tokenizer, ppl_device,\n",
        "                meta_model=None,\n",
        "                team_name=\"myteam\", lang=\"en\"):\n",
        "    \"\"\"\n",
        "    Complete bot detection pipeline for a new dataset (Enhanced v2).\n",
        "    Includes: basic + advanced temporal + stylometry + NLP + cross-user + clustering features.\n",
        "    Uses stacking ensemble if meta_model is provided.\n",
        "    Input: path to dataset JSON\n",
        "    Output: saves bot IDs to a text file and returns the list\n",
        "    \"\"\"\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"PROCESSING: {json_path}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # Step 1: Load data\n",
        "    print(\"[1/7] Loading data...\")\n",
        "    with open(json_path, 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "    posts_df = pd.DataFrame(data['posts'])\n",
        "    users_df = pd.DataFrame(data['users'])\n",
        "    ds_lang = data.get('lang', lang)\n",
        "    print(f\"  {len(users_df)} users, {len(posts_df)} posts, language: {ds_lang}\")\n",
        "\n",
        "    # Step 2: Basic features\n",
        "    print(\"[2/7] Extracting basic features...\")\n",
        "    basic = extract_all_basic_features(posts_df, users_df)\n",
        "\n",
        "    # Step 3: Advanced temporal features (NEW)\n",
        "    print(\"[3/7] Extracting advanced temporal features...\")\n",
        "    temporal_results = []\n",
        "    for _, user in users_df.iterrows():\n",
        "        uid = user['id']\n",
        "        user_posts = posts_df[posts_df['author_id'] == uid]\n",
        "        f = {'user_id': uid}\n",
        "        f.update(extract_advanced_temporal_features(user_posts))\n",
        "        temporal_results.append(f)\n",
        "    temporal_feats = pd.DataFrame(temporal_results)\n",
        "\n",
        "    # Step 4: Stylometry features (NEW)\n",
        "    print(\"[4/7] Extracting stylometry features...\")\n",
        "    stylo_results = []\n",
        "    for _, user in users_df.iterrows():\n",
        "        uid = user['id']\n",
        "        user_posts = posts_df[posts_df['author_id'] == uid]\n",
        "        f = {'user_id': uid}\n",
        "        f.update(extract_stylometry_features(user_posts))\n",
        "        stylo_results.append(f)\n",
        "    stylo_feats = pd.DataFrame(stylo_results)\n",
        "\n",
        "    # Step 5: NLP features\n",
        "    print(\"[5/7] Computing NLP features (embeddings + perplexity)...\")\n",
        "    emb_feats, mean_embs = extract_embedding_features(posts_df, users_df, model=embed_model)\n",
        "    ppl_feats = extract_perplexity_features(posts_df, users_df, ppl_model, ppl_tokenizer, ppl_device)\n",
        "    nlp_feat = emb_feats.merge(ppl_feats, on='user_id', how='outer')\n",
        "\n",
        "    # Step 6: Cross-user + clustering features\n",
        "    print(\"[6/7] Computing cross-user + clustering features...\")\n",
        "    cross_feats = extract_cross_user_features(mean_embs) if mean_embs else pd.DataFrame()\n",
        "    cluster_feats = extract_clustering_features(mean_embs, posts_df, users_df) if mean_embs else pd.DataFrame()\n",
        "\n",
        "    # Combine all\n",
        "    df = basic.copy()\n",
        "    df = df.merge(temporal_feats, on='user_id', how='left')\n",
        "    df = df.merge(stylo_feats, on='user_id', how='left')\n",
        "    df = df.merge(nlp_feat, on='user_id', how='left')\n",
        "    if not cross_feats.empty:\n",
        "        df = df.merge(cross_feats, on='user_id', how='left')\n",
        "    if not cluster_feats.empty:\n",
        "        df = df.merge(cluster_feats, on='user_id', how='left')\n",
        "    df = df.fillna(0)\n",
        "\n",
        "    # Ensure feature columns match training\n",
        "    for col in feature_columns:\n",
        "        if col not in df.columns:\n",
        "            df[col] = 0\n",
        "    X = df[feature_columns].values\n",
        "\n",
        "    # Step 7: Predict (with stacking if available)\n",
        "    print(\"[7/7] Predicting (stacking ensemble)...\")\n",
        "    proba = ensemble_predict_proba(models, X, meta_model=meta_model)\n",
        "    predictions = (proba >= threshold).astype(int)\n",
        "\n",
        "    # Get bot user IDs\n",
        "    bot_user_ids = df.loc[predictions == 1, 'user_id'].tolist()\n",
        "\n",
        "    # Save to file\n",
        "    output_filename = f\"{team_name}.detections.{ds_lang}.txt\"\n",
        "    with open(output_filename, 'w') as f:\n",
        "        for uid in bot_user_ids:\n",
        "            f.write(f\"{uid}\\n\")\n",
        "\n",
        "    print(f\"\\n  Detected {len(bot_user_ids)} bots out of {len(users_df)} users\")\n",
        "    print(f\"  Results saved to: {output_filename}\")\n",
        "\n",
        "    # Also print probability distribution for manual inspection\n",
        "    print(f\"\\n  Probability distribution:\")\n",
        "    print(f\"    Mean: {np.mean(proba):.4f}\")\n",
        "    print(f\"    > 0.3: {(proba > 0.3).sum()} users\")\n",
        "    print(f\"    > 0.5: {(proba > 0.5).sum()} users\")\n",
        "    print(f\"    > 0.7: {(proba > 0.7).sum()} users\")\n",
        "    print(f\"    > 0.9: {(proba > 0.9).sum()} users\")\n",
        "\n",
        "    return bot_user_ids, proba, df\n",
        "\n",
        "print(\"Final pipeline defined.\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "4d9a7d4a"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 8: Validate Pipeline on Practice Data\n",
        "\n",
        "Run the full pipeline on practice datasets to verify it works correctly and check the competition score."
      ],
      "id": "9314a780"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Validate on practice dataset 32 (English) using model trained on 30\n",
        "print(\"VALIDATION: Using Train-30 model on Dataset 32 (with Stacking)\")\n",
        "val_json = os.path.join(DATA_DIR, 'dataset.posts&users.32.json')\n",
        "val_bots = os.path.join(DATA_DIR, 'dataset.bots.32.txt')\n",
        "\n",
        "detected_bots, probas, result_df = detect_bots(\n",
        "    val_json,\n",
        "    models=trained_models[(30, 32)],\n",
        "    threshold=cv_results[(30, 32)]['threshold'],\n",
        "    feature_columns=[c for c in feature_cols if c in feature_dfs[30].columns],\n",
        "    embed_model=embed_model,\n",
        "    ppl_model=ppl_model, ppl_tokenizer=ppl_tokenizer, ppl_device=ppl_device,\n",
        "    meta_model=trained_meta_models.get((30, 32)),\n",
        "    team_name=\"validation\", lang=\"en\"\n",
        ")\n",
        "\n",
        "# Check against known bots\n",
        "with open(val_bots, 'r') as f:\n",
        "    true_bots = set(line.strip() for line in f if line.strip())\n",
        "\n",
        "detected_set = set(detected_bots)\n",
        "tp = len(detected_set & true_bots)\n",
        "fp = len(detected_set - true_bots)\n",
        "fn = len(true_bots - detected_set)\n",
        "score = 4 * tp - 1 * fn - 2 * fp\n",
        "\n",
        "print(f\"\\nVALIDATION RESULTS:\")\n",
        "print(f\"  True bots: {len(true_bots)}\")\n",
        "print(f\"  Detected bots: {len(detected_set)}\")\n",
        "print(f\"  TP: {tp}, FP: {fp}, FN: {fn}\")\n",
        "print(f\"  Competition Score: {score}\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "6c827d08"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 9: Final Evaluation (Run on Competition Day)\n",
        "\n",
        "On **Feb 14, 12:00 PM EST**, upload the new evaluation dataset JSON and run this cell.\n",
        "Change `TEAM_NAME` to your team name before running!"
      ],
      "id": "7cee7814"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# FINAL EVALUATION - CHANGE THESE BEFORE RUNNING\n",
        "# ============================================================\n",
        "TEAM_NAME = \"myteam\"  # <-- CHANGE THIS to your team name\n",
        "\n",
        "# Upload the final evaluation dataset to Colab/Drive first, then set paths:\n",
        "EVAL_EN_JSON = os.path.join(DATA_DIR, 'final_eval_en.json')  # <-- English eval dataset path\n",
        "EVAL_FR_JSON = os.path.join(DATA_DIR, 'final_eval_fr.json')  # <-- French eval dataset path\n",
        "\n",
        "# --- Run English detection (with stacking) ---\n",
        "if os.path.exists(EVAL_EN_JSON):\n",
        "    print(\"Running English bot detection (with stacking)...\")\n",
        "    en_bots, en_proba, en_df = detect_bots(\n",
        "        EVAL_EN_JSON,\n",
        "        models=final_en_models,\n",
        "        threshold=final_en_threshold,\n",
        "        feature_columns=common_cols_en,\n",
        "        embed_model=embed_model,\n",
        "        ppl_model=ppl_model, ppl_tokenizer=ppl_tokenizer, ppl_device=ppl_device,\n",
        "        meta_model=final_en_meta,\n",
        "        team_name=TEAM_NAME, lang=\"en\"\n",
        "    )\n",
        "else:\n",
        "    print(f\"English evaluation file not found: {EVAL_EN_JSON}\")\n",
        "\n",
        "# --- Run French detection (with stacking) ---\n",
        "if os.path.exists(EVAL_FR_JSON):\n",
        "    print(\"\\nRunning French bot detection (with stacking)...\")\n",
        "    fr_bots, fr_proba, fr_df = detect_bots(\n",
        "        EVAL_FR_JSON,\n",
        "        models=final_fr_models,\n",
        "        threshold=final_fr_threshold,\n",
        "        feature_columns=common_cols_fr,\n",
        "        embed_model=embed_model,\n",
        "        ppl_model=ppl_model, ppl_tokenizer=ppl_tokenizer, ppl_device=ppl_device,\n",
        "        meta_model=final_fr_meta,\n",
        "        team_name=TEAM_NAME, lang=\"fr\"\n",
        "    )\n",
        "else:\n",
        "    print(f\"French evaluation file not found: {EVAL_FR_JSON}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"DONE! Check the output files:\")\n",
        "print(f\"  English: {TEAM_NAME}.detections.en.txt\")\n",
        "print(f\"  French:  {TEAM_NAME}.detections.fr.txt\")\n",
        "print(f\"\\nSubmit these files to: bot.or.not.competition.adm@gmail.com\")\n",
        "print(f\"Deadline: Feb 14, 2026, 1:00 PM EST\")\n",
        "print(\"=\"*60)"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "b8b0693b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Running locally - files are already saved in the working directory\n",
        "for fname in [f\"{TEAM_NAME}.detections.en.txt\", f\"{TEAM_NAME}.detections.fr.txt\"]:\n",
        "    if os.path.exists(fname):\n",
        "        print(f\"File saved: {fname}\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "2aeea147"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}